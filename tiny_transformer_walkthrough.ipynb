{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1927f-b65a-4b02-a45a-d8a87aa7b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib ipympl\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "import math \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "#local imports\n",
    "from TransformerModules import Modelconfig, Block, MLP, CausalSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc7b6b-355f-480e-bebd-e169e1d42018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionOnlyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config \n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wce = nn.Embedding(config.vocab_size, config.n_embed),\n",
    "                wpe = nn.Embedding(config.block_size, config.n_embed),\n",
    "                h = CausalSelfAttention(config)\n",
    "                )\n",
    "            )\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of lenght {T}, block size is {self.config.block_size}\"\n",
    "        \n",
    "        char_emb = self.transformer.wce(idx)\n",
    "        pos_emb = self.transformer.wpe(torch.arange(0, T, dtype = torch.long))\n",
    "        \n",
    "        x = char_emb + pos_emb\n",
    "        \n",
    "        x = x+self.transformer.h(x)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokes):\n",
    "\n",
    "        for _ in range(max_new_tokes):\n",
    "            idx_cond = idx[:, -self.config.block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1698a-8a5e-44f3-96d6-c21df8923bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools to build the basic A, B, C, A, B, C, ... patterned dataset\n",
    "data = list('ABC'*1000)\n",
    "c_to_i = {'A':0, 'B':1, 'C':2}\n",
    "i_to_c = {i:c for c,i in c_to_i.items()}\n",
    "encode = lambda s: [c_to_i[c] for c in s] \n",
    "decode = lambda l: ''.join([i_to_c[i] for i in l])\n",
    "\n",
    "\n",
    "i_data = [c_to_i[c] for c in data]\n",
    "i_data = torch.tensor(i_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7a460-9ae3-4900-9187-9ee17a9533e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of training data\n",
    "def get_batch(model, batch_size):\n",
    "\tblock_size = model.config.block_size\n",
    "\tix = torch.randint(len(i_data)-block_size, (batch_size,))\n",
    "\tx = torch.stack([i_data[i:i+block_size] for i in ix])\n",
    "\ty = torch.stack([i_data[i+1:i+block_size+1] for i in ix])\n",
    "\treturn x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab6fcc-3ba1-49f6-ab3c-8ca910174433",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, train_data, val_data, eval_iters):\n",
    "\tout = {}\n",
    "\tdata = {'train': train_data, 'val': val_data}\n",
    "\tmodel.eval()\n",
    "\tfor split in ['train', 'val']:\n",
    "\t\tlosses = torch.zeros(eval_iters)\n",
    "\t\tfor k in range(eval_iters):\n",
    "\t\t\tX, Y = get_batch(model, batch_size)\n",
    "\t\t\tlogits, loss = model(X, Y)\n",
    "\t\t\tlosses[k] = loss.item()\n",
    "\t\tout[split] = losses.mean()\n",
    "\tmodel.train()\n",
    "\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf2113-aba8-4abd-a167-5aa832ed4968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, batch_size, max_iters, eval_interval, train_data, val_data, eval_iters):\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tfor iter in range(max_iters):\n",
    "\n",
    "\t\tif iter % eval_interval == 0:\n",
    "\t\t\tlosses = estimate_loss(model, batch_size, train_data, val_data, eval_iters)\n",
    "\t\t\tprint(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "\t\t# sample a batch of data\n",
    "\t\txb, yb = get_batch(model, batch_size)\n",
    "\n",
    "\t\t# evaluation the loss\n",
    "\t\tlogits, loss = model(xb, yb)\n",
    "\t\toptimizer.zero_grad(set_to_none=True)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\tlosses = estimate_loss(model, batch_size, train_data, val_data, eval_iters)\n",
    "\tprint(f\"step {max_iters}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\tprint(f\"Took {(time.time() - start_time)//60} minutes to train\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bca23-07e4-4757-be3e-5231f335d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinyconfig = Modelconfig\n",
    "tinyconfig.block_size = 3\n",
    "tinyconfig.vocab_size = 3\n",
    "tinyconfig.n_layer = 1\n",
    "tinyconfig.n_head = 1\n",
    "tinyconfig.n_embed = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018fd7f-9f6c-4618-9039-f687a7a42bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "# torch.manual_seed(2345)\n",
    "one_layer_model = AttentionOnlyModel(tinyconfig)\n",
    "optimizer = torch.optim.AdamW(one_layer_model.parameters(), )\n",
    "\n",
    "max_iters=5000\n",
    "# eval_interval=max_iters//100\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5fcbbb-26b2-4712-b04f-3f8a9e69aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(one_layer_model, optimizer, batch_size, max_iters, eval_interval, i_data, i_data, eval_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e67212-8411-4c30-8394-820175f6071a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52efad4e-ccc7-42f8-85fe-0bfc28dda853",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_tensor in one_layer_model.state_dict():\n",
    "\t\tprint(param_tensor, \"\\t\", one_layer_model.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6872639-26fe-4a10-865e-8cc0660362d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the embedding vectors\n",
    "emb_weights = one_layer_model.transformer.wce.weight.detach().numpy()\n",
    "pos_weights = one_layer_model.transformer.wpe.weight.detach().numpy()\n",
    "emb_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd722d-7086-4bb7-ad05-06b9efc75152",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = np.array([[0,0,0],[0,0,0]])\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (14,8))\n",
    "#plot 1\n",
    "ax1.quiver(*origin, emb_weights[:,0], emb_weights[:,1], color=['r','b','g'], angles='xy', scale_units='xy', scale=1)\n",
    "# ax1.xaxis.set_ticks([])\n",
    "# ax1.yaxis.set_ticks([])\n",
    "ax1.axis([-2,2,-2,2])\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_title('Token Embeddings, A ~ red, B ~ blue, C ~ green')\n",
    "\n",
    "#plot 2\n",
    "ax2.quiver(*origin, pos_weights[:,0], pos_weights[:,1], color=['r','b','g'], angles='xy', scale_units='xy', scale=1)\n",
    "# ax2.xaxis.set_ticks([])\n",
    "# ax2.yaxis.set_ticks([])\n",
    "ax2.axis([-2,2,-2,2])\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_title('Position Embeddings, 0 ~ red, 1 ~ blue, 2 ~ green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fb004-1451-4456-bef6-26645b195d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_x1 = emb_weights + pos_weights\n",
    "print(emb_x1)\n",
    "plt.clf() #clear current figure in order to start a new one\n",
    "plt.plot(figsize = (14,8))\n",
    "plt.quiver(*origin, emb_x1[:,0], emb_x1[:,1], color=['r','b','g'], angles='xy', scale_units='xy', scale=1)\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "plt.axis([-3,3,-3,3])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.title('Result of Embedding Layer, A at pos 0:red, B at pos 1:blue, C at pos 2:green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161cc450-078d-43e6-8e60-fba74022ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss= one_layer_model(torch.zeros((1,1), dtype=torch.long))\n",
    "print(logits.dtype)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093ccda-b847-4359-a198-72436b0fc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,1), dtype = torch.long)\n",
    "wce_x = one_layer_model.transformer.wce(x)\n",
    "wpe_x = one_layer_model.transformer.wpe(torch.arange(0, 1, dtype = torch.long))\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f8e3c-ade9-49cf-b86f-3fbe65b8c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weight embedding\", \"\\t\", wce_x)\n",
    "print(\"Position embedding\", \"\\t\", wpe_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833a033-818f-42cf-a669-cd4430c45325",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = one_layer_model.transformer.h.c_attn.weight.transpose(0,1)\n",
    "bs = one_layer_model.transformer.h.c_attn.bias\n",
    "wt.split(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b06816-e1da-4a2f-91a9-c40905178d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_x = wce_x + wpe_x\n",
    "print(\"Embedding\", \"\\t\", embed_x, embed_x.dtype)\n",
    "transformer_x = one_layer_model.transformer.h.c_attn(embed_x)\n",
    "q, k, v = transformer_x.split(2, dim=2)\n",
    "print(\"Project x into transformer\", \"\\t\", transformer_x)\n",
    "print(\"Q projection:\", \"\\t\", q)\n",
    "print(\"K projection:\", \"\\t\", k)\n",
    "print(\"V projection:\", \"\\t\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3406bd5-abbf-4bc2-9da5-e95fd9b6587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_x @ wt + bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c2a28d-cad5-4a42-9416-ec9b60c16945",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.arange(0,3, dtype=torch.long).view(1,3)\n",
    "wte_x1 = one_layer_model.transformer.wce(x1)\n",
    "wpe_x1 = one_layer_model.transformer.wpe(torch.arange(0,3, dtype = torch.long))\n",
    "print(wte_x1, wpe_x1)\n",
    "embed_x1 = wte_x1+wpe_x1\n",
    "print(embed_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98375b90-9444-45a4-af78-b79d701c0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_x = one_layer_model.transformer.h.c_attn(embed_x1)\n",
    "q, k, v = transformer_x.split(2, dim=2)\n",
    "print(\"Project x into transformer\", \"\\n\", transformer_x)\n",
    "print(\"Q projection:\", \"\\n\", q)\n",
    "print(\"K projection:\", \"\\n\", k)\n",
    "print(\"V projection:\", \"\\n\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed48c37-edbc-49ee-a344-876c1ad80b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Q, K, V weights: ', one_layer_model.transformer.h.c_attn.weight)\n",
    "print('Q, K, V biases: ', one_layer_model.transformer.h.c_attn.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219741a2-fde4-4d49-aaf8-31bec14f06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed_x1 @ wt)\n",
    "print(embed_x1 @ wt + bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c610b-bd89-4c7a-8dae-aa65fd44f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer_model(torch.arange(0,3, dtype=torch.long).view(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2257894-bbdb-4647-a3a5-849ba75697b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_proj = one_layer_model.lm_head.weight\n",
    "out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b59565-9319-4d36-b546-424e6eadd7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = np.linalg.svd(out_proj.detach().numpy(), full_matrices=False)\n",
    "print(\"U = \\n\", U)\n",
    "print(\"Sigma = \\n\", np.diag(S))\n",
    "print(\"V = \\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdddef4-8acd-49e6-bb9e-a6077855cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(np.matmul(np.diag(S), np.matrix_transpose(V)))\n",
    "print(eigenvalues)\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f635549-31ea-4d57-8f0a-83a5adcd76be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(U, np.matmul(np.diag(S), np.matrix_transpose(V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65b69c-3993-4690-9d65-e5f991c81dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "n=1000\n",
    "theta = np.linspace(0, 2*np.pi, n, endpoint=False)\n",
    "x = np.cos(theta)\n",
    "y = np.sin(theta)\n",
    "#colormap \n",
    "cmap = plt.get_cmap(None)\n",
    "\n",
    "U_scaled = U @ np.diag(S)\n",
    "\n",
    "A = out_proj.detach().numpy()\n",
    "XY = np.stack([x,y])\n",
    "XYZ = np.matmul(A, XY)\n",
    "X, Y, Z = XYZ[0], XYZ[1], XYZ[2]\n",
    "\n",
    "fig = plt.figure(figsize = (14,8))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.scatter(x,y, c=theta, cmap=cmap)\n",
    "ax1.axis([-2,2,-2,2])\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
    "ax2.scatter(X, Y, Z, c=theta, cmap=cmap)\n",
    "ax2.quiver(np.array([0,0]), np.array([0,0]), np.array([0,0]), U_scaled[0], U_scaled[1], U_scaled[2])\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_zlabel('z')\n",
    "ax2.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d84144-42f7-400a-9e4a-efeb5b6f1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "V @ V.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a74f7-e049-46dc-9e57-74303cd0f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82658852-1e78-4251-ac99-43495789aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    ".9*75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec15d3-d5e1-4c3d-843b-b93a506ba846",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.genfromtxt('Midterm_1_scores_only.csv', delimiter=',')\n",
    "x = scores[1:]/75*100\n",
    "y = np.sqrt(x)*10\n",
    "z = x\n",
    "w = 0.75*x+25\n",
    "percent_a = lambda x : 100 * (len(x[x>=90])/len(x))\n",
    "percent_b = lambda x : 100 * (len(x[np.logical_and(x < 90, x>= 75)]) / len(x))\n",
    "percent_c = lambda x : 100 * (len(x[np.logical_and(x < 75, x>= 50)]) / len(x))\n",
    "percent_d = lambda x : 100 * (len(x[np.logical_and(x < 50, x>= 30)]) / len(x))\n",
    "\n",
    "percent_b_alt = lambda x : 100 * (len(x[np.logical_and(x < 90, x>= 80)]) / len(x))\n",
    "percent_c_alt = lambda x : 100 * (len(x[np.logical_and(x < 80, x>= 70)]) / len(x))\n",
    "percent_d_alt = lambda x : 100 * (len(x[np.logical_and(x < 70, x>= 60)]) / len(x))\n",
    "percent_f_alt = lambda x : 100 * (len(x[x<30]) / len(x))\n",
    "print(percent_a(x), percent_b(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c01c5-30de-4112-b04e-46038b5a7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(14,8))\n",
    "axs[0,0].hist(x, edgecolor='white')\n",
    "axs[0,0].axvline(x=30, color='green', label='A cutoff')\n",
    "axs[0,0].set_title(f\"A: {percent_a(x):.1f}%, B: {percent_b(x):.1f}%, C: {percent_c(x):.1f}%, D: {percent_d(x):.1f}%\")\n",
    "\n",
    "axs[0,1].hist(y, edgecolor='white')\n",
    "axs[0,1].axvline(x=55, color='green', label='A cutoff')\n",
    "axs[0,1].set_title(f\"A: {percent_a(y):.1f}%, B: {percent_b(y):.1f}% C: {percent_c(y):.1f}%, D: {percent_d(y):.1f}%\")\n",
    "\n",
    "axs[1,0].hist(z, edgecolor='white')\n",
    "axs[1,0].axvline(x=30, color='green', label='A cutoff')\n",
    "axs[1,0].set_title(f\"A: {percent_a(z):.1f}%, B: {percent_b_alt(z):.1f}% C: {percent_c_alt(z):.1f}%, D: {percent_d_alt(z):.1f}%, Fail: {percent_f_alt(z):.1f}\")\n",
    "\n",
    "axs[1,1].hist(w, edgecolor='white')\n",
    "axs[1,1].axvline(x=55 , color='green', label='A cutoff')\n",
    "axs[1,1].set_title(f\"A: {percent_a(w):.1f}%, B: {percent_b(w):.1f}% C: {percent_c(w):.1f}%, D: {percent_d(w):.1f}%\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7817e87a-b63e-4ad1-96b7-dce1f600b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
